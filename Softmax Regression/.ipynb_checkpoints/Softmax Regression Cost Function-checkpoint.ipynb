{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Regression : 3개 이상의 선택지로부터 1개를 선택하는 문제인 다중 클래스 분류(Multi-Class classification)를 풀기 위해 사용\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff4e766e8d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n",
      "tensor([[0.2167, 0.2328, 0.2276, 0.2179, 0.1050],\n",
      "        [0.1563, 0.2923, 0.1285, 0.2960, 0.1269],\n",
      "        [0.1404, 0.2549, 0.1394, 0.1780, 0.2872]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([2, 0, 1])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[2],\n",
      "        [0],\n",
      "        [1]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "tensor(1.5677, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# <파이토치로 소프트맥스의 비용 함수 구현하기 (low-level)>\n",
    "\n",
    "z = torch.FloatTensor([1, 2, 3])\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)\n",
    "print(hypothesis.sum()) # 원소들의 값의 합 = 1\n",
    "\n",
    "z = torch.rand(3, 5, requires_grad=True) # 3x5 행렬\n",
    "hypothesis = F.softmax(z, dim=1) # dim=1 : 두번째 차원에 대해서 소프트맥스 함수를 적용한다는 의미 (각 샘플에 대해서 소프트맥스 함수를 적용하여야 하므로)\n",
    "print(hypothesis)\n",
    "\n",
    "y = torch.randint(5, size = (3,)).long() #  torch.randint() : 주어진 범위 내의 정수를 균등하게 생성\n",
    "print(y)\n",
    "\n",
    "#  one-hot encoding이 된 정답 텐서 생성\n",
    "y_one_hot = torch.zeros_like(hypothesis) # torch.zeros_like() : 0으로 이루어진 텐서 생성. 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "print(y_one_hot)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # scatter(dim, index, src) : dim 차원 기준으로 index 위치의 값을 src로 바꾼다.\n",
    "print(y.unsqueeze(1)) # y.unsqueeze(1) : (3,)의 크기를 가졌던 y 텐서 => (3 × 1) 텐서\n",
    "print(y_one_hot)\n",
    "\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5291, -1.4575, -1.4801, -1.5238, -2.2542],\n",
      "        [-1.8563, -1.2300, -2.0517, -1.2173, -2.0642],\n",
      "        [-1.9634, -1.3667, -1.9704, -1.7257, -1.2474]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5677, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# <파이토치로 소프트맥스의 비용 함수 구현하기 (high-level)>\n",
    "\n",
    "# 파이토치에서는 두 개의 함수를 결합한 F.log_softmax()라는 도구를 제공\n",
    "print(F.log_softmax(z, dim=1)) # torch.log(F.softmax(z, dim=1))\n",
    "\n",
    "# F.log_softmax() + F.nll_loss() = F.cross_entropy()\n",
    "# nll : Negative Log Likelihood\n",
    "print(F.nll_loss(F.log_softmax(z, dim=1), y)) # print((y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean())\n",
    "print(F.cross_entropy(z, y)) # F.cross_entropy는 비용 함수에 소프트맥스 함수까지 포함하고 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
